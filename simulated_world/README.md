# Simulator data

## Description of the data
The data from the simulator provides for each shot of the world: 
- Original image
- Flooded image 
- Depth image
- Semantic segmentation image
- json file with camera parameters

### Depth images

The depth maps are provided as RGBA images. Depth is encoded in the the following way: 
 - The information from the simulator is (1 - LinearDepth (in [0,1])).   
 `far` corresponds to the furthest distance to the camera included in the depth map. 
        `LinearDepth * far` gives the real metric distance to the camera. 
-  depth is first divided in 31 slices encoded in R channel with values ranging from 0 to 247 
- each slice is divided again in 31 slices, whose value is encoded in G channel
- each of the G slices is divided into 256 slices, encoded in B channel
    In total, we have a discretization of depth into `N = 31*31*256 - 1` possible values, each value covering a range of 
    far/N meters.   
    Note that, what we encode here is  `1 - LinearDepth` so that the furthest point is [0,0,0] (that is sky) 
    and the closest point[255,255,255] 
    The metric distance associated to a pixel whose depth is (R,G,B) is : 
    d = (far/N) * [((247 - R)//8)*256*31 + ((247 - G)//8)*256 + (255 - B)]  
    This is the same as :
    d = far* ( 1 - ((R//8)*256*31 + (G//8)*256 + B)/N )
      

### Segmentation images 
Segmentation masks are provided for the flooded version of the images. 
The following table provides the correspondence between classes and colors: 

 
| Label | Description |  RGBA | Color |  
| ----- | ----- | ----- | ----- | 
| Water|Water generated by the simulator    |[0, 0, 255, 255] |![#0000ff](https://placehold.it/15/0000ff/000000?text=+)
|Ground|Horizontal ground-level structures (road, roundabouts, parking)  |[55, 55, 55, 255] |![#373737](https://placehold.it/15/373737/000000?text=+)
| Building|Buildings, walls, fences| [255, 212, 0, 255]|![#ffd400](https://placehold.it/15/ffd400/000000?text=+)
|Traffic items| Poles, traffic signs, traffic lights | [0, 255, 255, 255]|![#00ffff](https://placehold.it/15/00ffff/000000?text=+)
|Vegetation| Tree, hedge, all kinds of vertical vegetation | [0, 255, 0, 255]|![#00ff00](https://placehold.it/15/00ff00/000000?text=+)
|Terrain| Grass, all kinds of vertical vegetation, soil, sand | [255, 97, 0, 255] | ![#ff6100](https://placehold.it/15/ff6100/000000?text=+)
|Sky| Open sky | [8, 19, 49, 255] | ![#081331](https://placehold.it/15/081331/000000?text=+)
|Car| This includes only cars | [255, 0, 0, 255] | ![#ff0000](https://placehold.it/15/ff0000/000000?text=+)
|Trees| Trees are seen as 2D in Unity and not segmented |  [0, 0, 0, 0]
|Truck| Vehicle with greater dimensions  than car (fixed threshold TBD)
|Person| Not in the dataset for now| 

### JSON files

The json files contain the following information:
- `CameraPosition`: camera *absolute* coordinates  in meters- the origin is not the ground but the origin of the simulated world
- `CameraRotation`: pitch (x) , yaw (y), roll (z) in degrees from 0 to 360 (for pitch the direction of the rotation is from down to up)
- `CameraFar`: how far do we compute the depth map until
- `CameraFOV`: vertical field of view in degrees
- `WaterLevel`: absolute level of water in meters

## Preprocessing and recovering 3D metric coordinates

The metric depth map is recovered from the depth images according to the process presented above in the data description.
The pinhole camera model is applied to recover the 3D metric coordinates. 
However because some points are very very far away from the camera we end up with heights that can seem outliers to the distribution. 
Our goal is to find a reference 0 level(ideally ground, where the camera sits) in order to compute height.
 However, because of the aforementioned issue, we can't just shift the min z value to 0. 
 In a first approach, what we will do is take the min value on the first 20 meters and consider that the 0 must be in these first 20 meters.
